{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sh/9y2x7hyx76bghklsygtm6m000000gn/T/ipykernel_79007/35827986.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pandas.read_csv('processed_cyberbullying_tweets.csv')\n",
    "tweets = twitter_df['processed_tweet_text'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in tweets]\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11656152,  0.37438858,  0.3292683 ,  0.10302036,  0.24794626,\n",
       "       -0.6317315 , -0.12931946,  0.6597121 , -0.26665574, -0.6152894 ,\n",
       "       -0.48683625, -0.5928095 ,  0.09547028,  0.57950747,  0.33780646,\n",
       "       -0.38385776,  0.7272408 , -1.1879708 , -0.03241641, -0.37247065,\n",
       "        0.15060964,  0.45045048, -0.20282362, -0.00388904, -0.2818553 ,\n",
       "        0.9146622 , -0.43120676,  0.31137404, -0.6128463 , -0.6574089 ,\n",
       "        0.46194103,  0.3514842 , -0.6977897 , -0.07252809,  0.05028884,\n",
       "        0.05093319,  0.5308221 ,  0.01195968, -0.15001433, -0.32282862,\n",
       "       -0.2746927 ,  0.02712599,  0.59781885, -0.98294216, -0.0636916 ,\n",
       "       -0.39662546, -0.6538947 ,  0.17041467,  0.550579  ,  0.27867228,\n",
       "        0.14114442, -0.16514648, -0.056178  , -0.3345168 , -0.4168285 ,\n",
       "       -0.05789689,  0.8499715 ,  0.0343224 , -0.14778924, -0.001401  ,\n",
       "       -0.2966222 , -0.33313206, -0.01285759,  0.4332352 , -1.1464317 ,\n",
       "        0.1906514 ,  0.26931182, -0.34804922,  0.05302043,  1.3646038 ,\n",
       "       -0.12937161, -0.21270777,  0.26684043, -0.09592037,  0.36849216,\n",
       "        0.01850417,  0.5711679 , -0.4369371 , -0.32662097,  0.56732523,\n",
       "       -0.42661953,  0.41053528, -0.46753   ,  0.6488157 ,  0.30396912,\n",
       "        0.18915413, -0.7313321 ,  1.2922759 ,  0.02134548,  0.27334663,\n",
       "        0.33185947, -0.24838   ,  0.62122875,  0.27375665, -0.04498918,\n",
       "        0.8412236 ,  0.9046464 , -0.4956625 ,  0.1407095 ,  0.25381282],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52456"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(model.wv.key_to_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['word_embeddings'] = twitter_df.apply(lambda x : list(), axis=1)\n",
    "\n",
    "for index, row in twitter_df.iterrows():\n",
    "    for word in word_tokenize(str(row['processed_tweet_text']).lower()):\n",
    "        row['word_embeddings'].append(model.wv[word])\n",
    "    while len(row['word_embeddings']) < 50:\n",
    "        row['word_embeddings'].append([0]*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twitter_df.loc[0, 'word_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twitter_df.loc[0, 'word_embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03375451,  0.01639242,  0.26653334, -0.15941752, -0.02850819,\n",
       "       -0.53289413,  0.06161923,  0.55192417,  0.04856513, -1.0466564 ,\n",
       "       -0.15115912, -1.2139022 ,  0.04472702,  0.23184012,  0.4745724 ,\n",
       "        0.033797  ,  0.39826915, -1.1947025 , -0.16768028, -0.7398385 ,\n",
       "        0.18270423,  0.37173277,  0.11691167, -0.3011429 ,  0.03884634,\n",
       "        1.1098007 , -0.67948616,  0.48499182, -0.5553805 , -0.46493334,\n",
       "        0.3885458 ,  0.28694513, -0.17727672, -0.15302575,  0.03137971,\n",
       "        0.30592123,  0.5192671 , -0.15145816, -0.06599976, -0.46803582,\n",
       "       -0.5047994 , -0.10696647,  0.50924563, -0.9958814 ,  0.15856008,\n",
       "       -0.5104042 , -0.27912298,  0.32230994,  0.71959764,  0.635307  ,\n",
       "        0.41909635, -0.32195383,  0.07035705, -0.07057319, -0.5673716 ,\n",
       "       -0.10604136,  0.8826875 , -0.02792183, -0.6713345 , -0.19214663,\n",
       "       -0.34260878, -0.01082203,  0.41187453,  0.8668792 , -0.940642  ,\n",
       "        0.13074401,  0.33475482,  0.20683964, -0.25828996,  1.4655592 ,\n",
       "       -0.26040158,  0.03321632,  0.21020581, -0.01735595,  0.9448628 ,\n",
       "        0.18679586,  0.6692927 , -0.42580932, -0.24514335,  0.5644343 ,\n",
       "       -0.46244076,  0.26726356, -0.49929526,  0.75700617,  0.47487926,\n",
       "       -0.1621902 , -0.54421014,  1.3038055 ,  0.35171512,  0.531591  ,\n",
       "        0.7526679 ,  0.24191085,  0.831331  ,  0.2151256 ,  0.2593659 ,\n",
       "        1.2022809 ,  1.2601784 , -0.4992712 , -0.18693036,  0.5724445 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = twitter_df['word_embeddings'].tolist()\n",
    "y = twitter_df['cyberbullying_type'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 100)           5245600   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               84480     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5338401 (20.36 MB)\n",
      "Trainable params: 5338401 (20.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "bilstm = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "bilstm.add(Embedding(input_dim=52456, output_dim=100, input_length=50))\n",
    "\n",
    "# Bidirectional LSTM layer\n",
    "bilstm.add(Bidirectional(LSTM(64)))\n",
    "\n",
    "# Additional Dense layer for more complex transformations\n",
    "bilstm.add(Dense(64, activation='relu'))  # Added intermediate Dense layer with 64 units\n",
    "\n",
    "# Output layer for binary classification\n",
    "bilstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "bilstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary to see the updated architecture\n",
    "bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
