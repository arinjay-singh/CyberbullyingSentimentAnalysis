{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sh/9y2x7hyx76bghklsygtm6m000000gn/T/ipykernel_80281/670781935.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "twitter_df = pd.read_csv('processed_cyberbullying_tweets.csv')\n",
    "tweets = twitter_df['processed_tweet_text'].astype(str).tolist()\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in tweets]\n",
    "model_w2v = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model_w2v.wv['food']\n",
    "\n",
    "\n",
    "vocab_size = len(model_w2v.wv.key_to_index)\n",
    "vocab_size\n",
    "\n",
    "twitter_df['word_embeddings'] = twitter_df.apply(lambda x : list(), axis=1)\n",
    "\n",
    "for index, row in twitter_df.iterrows():\n",
    "    for word in word_tokenize(str(row['processed_tweet_text']).lower()):\n",
    "        row['word_embeddings'].append(model_w2v.wv[word])\n",
    "    while len(row['word_embeddings']) < 50:\n",
    "        row['word_embeddings'].append([0]*100)\n",
    "    \n",
    "\n",
    "len(twitter_df.loc[0, 'word_embeddings'])\n",
    "len(twitter_df.loc[0, 'word_embeddings'][0])\n",
    "\n",
    "\n",
    "model_w2v.wv['words']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = twitter_df['word_embeddings'].tolist()\n",
    "y = twitter_df['cyberbullying_type'].tolist()\n",
    "\n",
    "data_0 = twitter_df[twitter_df['cyberbullying_type'] == 0]\n",
    "data_1 = twitter_df[twitter_df['cyberbullying_type'] == 1]\n",
    "\n",
    "sampled_data_0 = data_0.sample(n=7000, random_state=42)\n",
    "sampled_data_1 = data_1.sample(n=20000, random_state=42)\n",
    "\n",
    "balanced_df = pd.concat([sampled_data_0, sampled_data_1])\n",
    "\n",
    "X_dec = balanced_df['word_embeddings'].tolist()\n",
    "y_dec = balanced_df['cyberbullying_type'].tolist()\n",
    "\n",
    "# Shuffle the resulting DataFrame to mix the examples of 0s and 1s\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dec, y_dec, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming X_train, X_test are lists of lists of word embeddings, and y_train, y_test are labels\n",
    "# Since the input data is already in the form of embeddings, we don't need an embedding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Padding sequences to ensure uniform input size\n",
    "max_length = max(max(len(seq) for seq in X_train), max(len(seq) for seq in X_test))\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_length, padding='post', dtype='float32')\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_length, padding='post', dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "237/237 [==============================] - 85s 349ms/step - loss: 0.3714 - accuracy: 0.8017 - val_loss: 0.3294 - val_accuracy: 0.8185\n",
      "Epoch 2/5\n",
      "237/237 [==============================] - 87s 367ms/step - loss: 0.3417 - accuracy: 0.8151 - val_loss: 0.3303 - val_accuracy: 0.8204\n",
      "Epoch 3/5\n",
      "237/237 [==============================] - 88s 369ms/step - loss: 0.3332 - accuracy: 0.8196 - val_loss: 0.3547 - val_accuracy: 0.8167\n",
      "Epoch 4/5\n",
      "237/237 [==============================] - 89s 377ms/step - loss: 0.3244 - accuracy: 0.8229 - val_loss: 0.3241 - val_accuracy: 0.8341\n",
      "Epoch 5/5\n",
      "237/237 [==============================] - 94s 398ms/step - loss: 0.3189 - accuracy: 0.8279 - val_loss: 0.3105 - val_accuracy: 0.8349\n",
      "254/254 [==============================] - 14s 57ms/step - loss: 0.3201 - accuracy: 0.8270\n",
      "Test Accuracy: 0.8270370364189148\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(max_length, 100)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' for multi-class classification\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Use 'categorical_crossentropy' for multi-class classification\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train_padded, np.array(y_train), epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(X_test_padded, np.array(y_test))\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'are', 'rather', 'ugly', ',', 'you', 'know', 'that', '?']\n",
      "[[[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [-0.15282576  0.36009213  0.42436928 ... -0.41772056 -0.05103829\n",
      "    0.33079168]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]]\n",
      "1/1 [==============================] - 1s 766ms/step\n",
      "[[0.6715172]]\n",
      "Sentence: 'You are rather ugly, you know that?'\n",
      "Predicted Sentiment: Negative (Confidence: 0.67)\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(sentence, model, word2vec_model):\n",
    "    # Tokenize and convert to embeddings\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    print(tokens)\n",
    "    embeddings = [word2vec_model.wv[token] if token in word2vec_model.wv else np.zeros(100) for token in tokens]\n",
    "    \n",
    "    # Pad the sequence\n",
    "    padded_embeddings = pad_sequences([embeddings], maxlen=max_length, padding='post', dtype='float32')\n",
    "    print(padded_embeddings)\n",
    "    # Predict\n",
    "    prediction = model.predict(padded_embeddings)\n",
    "    print(prediction)\n",
    "    # Assuming binary classification with a sigmoid output layer\n",
    "    predicted_label = 'Positive' if prediction[0][0] < 0.5 else 'Negative'\n",
    "    \n",
    "    return predicted_label, prediction[0][0]\n",
    "\n",
    "# Example usage\n",
    "test_sentence = \"You are rather ugly, you know that?\"\n",
    "predicted_label, confidence = predict_sentiment(test_sentence, model, model_w2v)  # 'model' refers to your trained LSTM model, and 'model' is your Word2Vec model\n",
    "\n",
    "print(f\"Sentence: '{test_sentence}'\")\n",
    "print(f\"Predicted Sentiment: {predicted_label} (Confidence: {confidence:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
